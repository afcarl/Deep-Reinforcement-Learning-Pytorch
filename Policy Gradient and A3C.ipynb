{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "from multiprocessing import Process\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, n_layers, size, output_size):\n",
    "        self.n_layers = n_layers\n",
    "        self.size = size\n",
    "        self.input_size = input_size\n",
    "        self.activation = activation\n",
    "        self.output_activ = output_activ\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    \n",
    "        # Create the nn Linear Layers\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.input_size, self.size)])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.output_layer = nn.Linear(self.size, self.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def path_length(path):\n",
    "    return len(path['reward'])\n",
    "\n",
    "def normalize(data, mean=0.0, std=1.0):\n",
    "    n_data = (data - np.mean(data)) / (np.std(data) + 1e-8)\n",
    "    return n_data * (std + 1e-8) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training the policy gradient\n",
    "def train_policy_gradient(exp_name, env_name, optimizer, critic_optimizer, n_epochs=100, gamma=1.0, min_timesteps_per_batch=1000, \n",
    "             max_path_length=None,\n",
    "             learning_rate=5e-3, \n",
    "             reward_to_go=True, \n",
    "             animate=True, \n",
    "             logdir=None, \n",
    "             normalize_advantages=True,\n",
    "             nn_baseline=False, \n",
    "             seed=0,\n",
    "             # network arguments\n",
    "             n_layers=1,\n",
    "             size=32):\n",
    "    \n",
    "    start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    critic_optimizer.zero_grad()\n",
    "    \n",
    "    # Setup the openai gym environment\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Is this env discrete or continuous ?\n",
    "    discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "    \n",
    "    # Maximum length of episodes\n",
    "    max_eps_length = max_path_length or env.spec.max_episode_steps\n",
    "    \n",
    "    #Observation and action sizes\n",
    "    ob_dim = env.observation_space.shape[0]\n",
    "    if discrete:\n",
    "        ac_dim = env.action_space.n\n",
    "    else:\n",
    "        ac_dim = env.action_space.shape[0]\n",
    "    \n",
    "    policy_network = MLP(input_size=ob_dim, n_layers=n_layers, size=size, output_size=ac_dim)      \n",
    "        \n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"********** Iteration %i ************\"%epoch)\n",
    "        \n",
    "        # Collect paths until we have enough timesteps\n",
    "        timesteps_this_batch = 0\n",
    "        paths = []\n",
    "        while True:\n",
    "            ob = env.reset()\n",
    "            obs, acs, rewards = [], [], []\n",
    "            log_probs = []\n",
    "            steps = 0\n",
    "            while True:\n",
    "                obs.append(ob)\n",
    "                if discrete:\n",
    "                    logits = policy_network(obs)\n",
    "                    sampled_ac = torch.multinomial(logits, 1)\n",
    "                    sampled_ac = sampled_ac.view((-1))\n",
    "                    cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "                    logprob = cross_entropy_loss(logits, ac)\n",
    "                else:\n",
    "                    mean = policy_network(obs)\n",
    "                    logstd = nn.Parameter(torch.zeros(shape=[1, ac_dim]))\n",
    "                    std = torch.exp(logstd)\n",
    "                    sampled_z = torch.randn(mean.shape)\n",
    "                    sampled_ac = mean+std*sampled_z\n",
    "                    z = (ac - mean)/std\n",
    "                    logprob = -0.5 * torch.add(z**2)\n",
    "                   \n",
    "                # Get the actions from the policy network\n",
    "                ac = sampled_ac\n",
    "                ac = ac[0]\n",
    "                acs.append(ac)\n",
    "                log_probs.append(logprob)\n",
    "                # Step through the simulator to get the next state and reward\n",
    "                ob, rew, done, _ = env.step(ac)\n",
    "                rewards.append(rew)\n",
    "                steps +=1 \n",
    "                if done or steps > max_path_length:\n",
    "                    break\n",
    "            path = {\"observation\" : np.array(obs), \n",
    "                    \"reward\" : np.array(rewards), \n",
    "                    \"action\" : np.array(acs),\n",
    "                   \"log_probs\": np.array(log_probs)}\n",
    "            paths.append(path)\n",
    "            timesteps_this_batch+=path_length(path)\n",
    "            if timesteps_this_batch > min_timesteps_per_batch:\n",
    "                break\n",
    "        total_timesteps += timesteps_this_batch  \n",
    "        ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
    "        ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
    "        log_probs = np.concatenate([path['log_probs'] for path in paths])\n",
    "        \n",
    "        # Computing the Q Values\n",
    "        q_n = []\n",
    "        for path in paths:\n",
    "            q = 0\n",
    "            q_path = []\n",
    "\n",
    "            # Dynamic programming over reversed path\n",
    "            for rew in reversed(path[\"reward\"]):\n",
    "                q = rew + gamma * q\n",
    "                q_path.append(q)\n",
    "            q_path.reverse()\n",
    "\n",
    "            # Append these q values\n",
    "            if not reward_to_go:\n",
    "                q_path = [q_path[0]] * len(q_path)\n",
    "        \n",
    "            q_n.extend(q_path)\n",
    "                \n",
    "        if nn_baseline:\n",
    "            b_n = value_function_approximator(ob_no)\n",
    "            b_n = normalize(b_n, np.mean(q_n), np.std(q_n))\n",
    "            adv_n = q_n - b_n\n",
    "        else:\n",
    "            adv_n = q_n\n",
    "            \n",
    "        if normalize_advantages:\n",
    "            adv_n = normalize(adv_n, 0.0, 1.0)\n",
    "            \n",
    "        if nn_baseline:\n",
    "            q_normalized_n = normalize(q_n)\n",
    "            value_function_approximator = MLP(input_size=ob_dim, n_layers=n_layers, size=size, output_size=1)\n",
    "            baseline_loss = nn.MSELoss()\n",
    "            loss_b = baseline_loss(value_function_approximator(obs), q_normalized_n)\n",
    "            loss_b.backward()\n",
    "            critic_optimizer.step()   \n",
    "            \n",
    "        \n",
    "        # Final loss function\n",
    "        loss = torch.mean(log_probs*adv)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
